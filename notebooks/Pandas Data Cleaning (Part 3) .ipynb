{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff801fb",
   "metadata": {},
   "source": [
    "# Data Cleaning with Python (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63d421",
   "metadata": {},
   "source": [
    "In this topic, we will learn about: \n",
    "1. **Feature Engineering**\n",
    "2. **Data Integration**\n",
    "\n",
    "These are two critical steps in preparing data for analysis and modeling. Feature engineering transforms raw data into meaningful features, while data integration combines data from multiple sources, enhancing the dataset’s depth and scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f2219",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Data\n",
    "\n",
    "First, we need to import the necessary libraries and load our dataset that was cleaned in Part 1. Pandas is the primary library we’ll use to manipulate our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('cleaned_data_3.csv')\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebd0d7",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating new features from existing data, which can help improve the quality and relevance of data for modeling and analysis.\n",
    "\n",
    "### a) Extracting Year, Month, and Day from Transaction Date\n",
    "\n",
    "Extracting `Year`, `Month`, and `Day` from a date column can help reveal patterns over time, such as seasonal trends or monthly cycles. This breakdown allows for detailed analysis of yearly trends, monthly fluctuations, and daily patterns, providing insights into time-based behaviors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c050ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to ensure that the Transaction date is in datetime format.\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80015963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, and day\n",
    "df['Year'] = df['Transaction Date'].dt.year\n",
    "df['Month'] = df['Transaction Date'].dt.month\n",
    "df['Day'] = df['Transaction Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f47c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0fbf50",
   "metadata": {},
   "source": [
    "### b) Creating a Loyalty Points per Transaction Feature\n",
    "\n",
    "The `Loyalty Points per Transaction` feature is a simple way to measure customer loyalty by assigning points based on customer spending in each transaction. This feature can be used to develop loyalty programs or identify high-value customers.\n",
    "\n",
    "- **Assign Loyalty Points per Transaction** based on the `Total Amount` (e.g., 1 point for every $10 spent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabfcca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define points conversion rate\n",
    "points_rate = 10  # 1 point for every $10 spent\n",
    "\n",
    "# Calculate loyalty points by dividing Total Amount by points_rate\n",
    "df['Loyalty Points'] = df['Total Amount'] / points_rate\n",
    "\n",
    "# Convert loyalty points to an integer value (rounding down)\n",
    "df['Loyalty Points'] = df['Loyalty Points'].astype(int)\n",
    "\n",
    "# Display the DataFrame with the new Loyalty Points feature\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4924",
   "metadata": {},
   "source": [
    "For analysis purposes, these loyalty points can be grouped by `Customer ID` to find the **total loyalty points per customer**. This allows businesses to assess customer loyalty on an individual level, identifying high-value customers based on their accumulated points across multiple transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27118db",
   "metadata": {},
   "source": [
    "## 2. Data Integration\n",
    "\n",
    "Data integration combines datasets from multiple sources to provide a more comprehensive view. This is useful for enriching our data with additional information and enabling deeper analysis.\n",
    "\n",
    "### Merging Datasets\n",
    "\n",
    "To integrate data, we can use `pd.merge()` to join datasets on a common column (e.g., `Customer ID`). This method aligns rows from each dataset based on the values in the specified column, allowing us to combine related data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f0990",
   "metadata": {},
   "source": [
    "### Load the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df2 = pd.read_csv('../inputs/datasets/raw/Customer_Demographics.xls')\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df, df2, on='CustomerID', how='inner')\n",
    "df_combined[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ff3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ae07b",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "`pd.merge()` joins two DataFrames (`df` and `df2`) based on the common column `Customer ID`.\n",
    "\n",
    "The `how` parameter determines the type of join, and it has the following options:\n",
    "\n",
    "- `how='inner'`: Only includes rows with matching values in both datasets (default behavior).\n",
    "- `how='left'`: Keeps all rows from the left DataFrame (`df`), filling in `NaN` for missing values from the right DataFrame (`df2`).\n",
    "- `how='right'`: Keeps all rows from the right DataFrame (`df2`), filling in `NaN` for missing values from the left DataFrame (`df`).\n",
    "- `how='outer'`: Includes all rows from both DataFrames, filling in `NaN` for any missing values.\n",
    "\n",
    "Using these options, you can tailor the merge to fit your data needs and create a unified dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c424623",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYdf1 = pd.DataFrame({\n",
    "  'name': ['John', 'Jane', 'Jim'],\n",
    "  'age': [28, 34, 29],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago']\n",
    "})\n",
    "\n",
    "MYdf2 = pd.DataFrame({\n",
    "  'name': ['Abert', 'Benny', 'Carlos','John', 'Jane', 'Jim'],\n",
    "  'age': [22, 33, 49,28, 34, 29],\n",
    "    'city': ['York', 'Newport', 'Coventry','New York', 'Los Angeles', 'Chicago']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYdf3 = pd.merge(MYdf1,MYdf2, how='outer', on='name')\n",
    "MYdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edf75d",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Data integration and feature engineering are crucial steps in preparing data for analysis. By merging datasets, we gain a complete view that uncovers insights hidden in isolated sources. Feature engineering allows us to transform variables and interpretability. Together, these techniques improve data quality and set the foundation for impactful, data-driven decisions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
